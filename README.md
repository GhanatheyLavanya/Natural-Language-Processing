# Natural-Language-Processing

Assignment-1: <br>
- Analyzed speech data using Python, calculating entropy (1.846 bits), cross-entropy, and KL divergence (0.154 bits). <br> 
- Built Bayesian and logistic regression models for spam detection, achieving 98% accuracy using Python, NLTK, Scikit-learn, and TfidfVectorizer. <br>
<br>
Assignment-2: <br>
- Applying overfitting strategies (L1 regularization, early stopping) results in better generalization but slightly lower accuracy, while models without these strategies risk overfitting. <br>
- Bidirectional LSTM improves performance by considering both past and future context, making it highly effective for tasks like sentiment analysis and sequence prediction. <br>
<br>
Assignment-3: <br>
- The Word2Vec model, trained on a spam dataset, reveals how words like 'say', 'said', and 'told' are closely related in meaning, with high cosine similarity scores showing strong connections between them. <br>
- By comparing FastText, GloVe, and Word2Vec embeddings, we explored how these models differ in capturing relationships between words, such as gender and geography, using methods like semantic distance and t-SNE visualizations. <br>
<br>
Assignment-4: <br>
- The BERT-based question-answering system leverages dynamic memory, allowing users to store and update contexts under unique keys, enabling real-time answers to user queries based on the most relevant information. <br>
- Despite challenges like memory management and performance issues, the system provides a robust foundation for real-time, adaptive question answering, with future plans to integrate advanced models and improve scalability.
